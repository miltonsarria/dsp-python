{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tutorial para entrenar redes neuronales\n",
    "======================\n",
    "\n",
    "Preparacion de datos\n",
    "-----------------------\n",
    "Inicialmente se busca que el estudiante conozca practicas comunes en manipulacion de datos\n",
    "\n",
    "Este tutorial usa la base de datos notMNIST para experimentar con el lenguaje python. Es una base de datos que contiene letras del alfabeto. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Inicialmente se deben importar todos los modulos necesarios\n",
    "# esto se debe hacer antes de ejecutar cualquier otra instruccion\n",
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import tarfile\n",
    "from IPython.display import display, Image\n",
    "from scipy import ndimage\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from six.moves import cPickle as pickle\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se debe descargar la base de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url = 'http://commondatastorage.googleapis.com/books1000/'\n",
    "last_percent_reported = None\n",
    "\n",
    "def download_progress_hook(count, blockSize, totalSize):\n",
    "\n",
    "  global last_percent_reported\n",
    "  percent = int(count * blockSize * 100 / totalSize)\n",
    "\n",
    "  if last_percent_reported != percent:\n",
    "    if percent % 5 == 0:\n",
    "      sys.stdout.write(\"%s%%\" % percent)\n",
    "      sys.stdout.flush()\n",
    "    else:\n",
    "      sys.stdout.write(\".\")\n",
    "      sys.stdout.flush()\n",
    "      \n",
    "    last_percent_reported = percent\n",
    "        \n",
    "def maybe_download(filename, expected_bytes, force=False):\n",
    "  \"\"\"Descargar archivos si no se encuentran o tienen tamano diferente al que deberian tener.\"\"\"\n",
    "  if force or not os.path.exists(filename):\n",
    "    print('Intentando descargar:', filename) \n",
    "    filename, _ = urlretrieve(url + filename, filename, reporthook=download_progress_hook)\n",
    "    print('\\n Descarga completa!')\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Datos encontrados y verificados', filename)\n",
    "  else:\n",
    "    raise Exception(\n",
    "      'No se pudo verificar ' + filename + '. Por favor usar un navegador para descargarlo')\n",
    "  return filename\n",
    "\n",
    "train_filename = maybe_download('notMNIST_large.tar.gz', 247336696)\n",
    "test_filename = maybe_download('notMNIST_small.tar.gz', 8458043)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se debe extraer la base de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "np.random.seed(133)\n",
    "\n",
    "def maybe_extract(filename, force=False):\n",
    "  root = os.path.splitext(os.path.splitext(filename)[0])[0]  # remove .tar.gz\n",
    "  if os.path.isdir(root) and not force:\n",
    "    # force=True: puede sobreescribir datos existentes\n",
    "    print('%s Ya existe - Omitir extraccion de %s.' % (root, filename))\n",
    "  else:\n",
    "    print('Extraer datos de %s. Esto puede demorar. Por favor espere.' % root)\n",
    "    tar = tarfile.open(filename)\n",
    "    sys.stdout.flush()\n",
    "    tar.extractall()\n",
    "    tar.close()\n",
    "  data_folders = [\n",
    "    os.path.join(root, d) for d in sorted(os.listdir(root))\n",
    "    if os.path.isdir(os.path.join(root, d))]\n",
    "  if len(data_folders) != num_classes:\n",
    "    raise Exception(\n",
    "      'ERROR: se esperaban %d directorios, uno por clase. Se encontraron %d .' % (\n",
    "        num_classes, len(data_folders)))\n",
    "  print(data_folders)\n",
    "  return data_folders\n",
    "  \n",
    "train_folders = maybe_extract(train_filename)\n",
    "test_folders = maybe_extract(test_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problema 1\n",
    "===========\n",
    "El estudiante debe verificar que los datos estan correctos y corresponden a imagenes. Visualizar un ejemplo de cada clase usando el paquete IPython.display.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for root in train_folders:\n",
    "    list_im=sorted(os.listdir(root))\n",
    "    indx=np.random.permutation(len(list_im))\n",
    "    im=root+'/'+list_im[indx[0]]\n",
    "    display(Image(filename=im))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuacion se organizan los datos de una forma mas manejable. Dependiendo de las capacidades del computador, se prefiere no cargar todos los datos en memoria, es mejor guardar cada clase en un archivo y cargarla cuando sea necesario. Se guardaran archivos de cada clase en un arreglo 3-D: (indice de la imagen, pixel x, pixel y), cada imagen se normaliza para que tenga media cero y desviacion estandar de aproximadamente 0.5, esto facilita el proceso de datos por los algoritmos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_size = 28  # Altura y ancho en pixeles.\n",
    "pixel_depth = 255.0  # Numero de niveles por pixel\n",
    "\n",
    "def load_letter(folder, min_num_images):\n",
    "  \n",
    "  image_files = os.listdir(folder)\n",
    "  dataset = np.ndarray(shape=(len(image_files), image_size, image_size),\n",
    "                         dtype=np.float32)\n",
    "  print(folder)\n",
    "  num_images = 0\n",
    "  for image in image_files:\n",
    "    image_file = os.path.join(folder, image)\n",
    "    try:\n",
    "      image_data = (ndimage.imread(image_file).astype(float) - \n",
    "                    pixel_depth / 2) / pixel_depth\n",
    "      if image_data.shape != (image_size, image_size):\n",
    "        raise Exception('Tamano de imagen no correcto: %s' % str(image_data.shape))\n",
    "      dataset[num_images, :, :] = image_data\n",
    "      num_images = num_images + 1\n",
    "    except IOError as e:\n",
    "      print('No se pudo leer:', image_file, ':', e, '- it\\'s ok, ignorando....')\n",
    "    \n",
    "  dataset = dataset[0:num_images, :, :]\n",
    "  if num_images < min_num_images:\n",
    "    raise Exception('No se satisface el numero minimo de imagenes: %d < %d' %\n",
    "                    (num_images, min_num_images))\n",
    "    \n",
    "  print('Dimensiones totales:', dataset.shape)\n",
    "  print('Media:', np.mean(dataset))\n",
    "  print('Desviacion estandar:', np.std(dataset))\n",
    "  return dataset\n",
    "        \n",
    "def maybe_pickle(data_folders, min_num_images_per_class, force=False):\n",
    "  dataset_names = []\n",
    "  for folder in data_folders:\n",
    "    set_filename = folder + '.pickle'\n",
    "    dataset_names.append(set_filename)\n",
    "    if os.path.exists(set_filename) and not force:\n",
    "      \n",
    "      print('%s Ya existe - Ignorando....' % set_filename)\n",
    "    else:\n",
    "      print('Pickling %s.' % set_filename)\n",
    "      dataset = load_letter(folder, min_num_images_per_class)\n",
    "      try:\n",
    "        with open(set_filename, 'wb') as f:\n",
    "          pickle.dump(dataset, f, pickle.HIGHEST_PROTOCOL)\n",
    "      except Exception as e:\n",
    "        print('No se puede guardar datos en: ', set_filename, ':', e)\n",
    "  \n",
    "  return dataset_names\n",
    "\n",
    "train_datasets = maybe_pickle(train_folders, 45000)\n",
    "test_datasets = maybe_pickle(test_folders, 1800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problema 2\n",
    "======\n",
    "Verificar que los datos todavia son correctos. El estudiante debe mostrar 10 muestras de cada clase seleccionadas de forma aleatoria. Para hacer esto se puede usar matplotlib.pyplot\n",
    "\n",
    "El estudiante debe tambien verificar que el numero de imagenes es balanceado en las clases. Mostrar cuantas muestras tiene cada clase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IM=np.zeros([2,282])\n",
    "nbr_samples=[]\n",
    "for root in train_datasets:\n",
    "    f=open(root,'r')\n",
    "    xx=pickle.load(f)\n",
    "    xx_root=np.zeros([28,2])    \n",
    "    indx=np.random.permutation(xx.shape[0])\n",
    "    nbr_samples.append(xx.shape[0])\n",
    "    for ii in range(10):\n",
    "         xx_root=np.hstack((xx_root,xx[indx[ii],:,:]))\n",
    "    IM=np.vstack((IM,xx_root))\n",
    "plt.imshow(IM,cmap=\"gray\")\n",
    "plt.show()\n",
    "\n",
    "print(nbr_samples)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separar datos de entrenamiento y datos para prueba y validacion\n",
    "======\n",
    "Dependiendo de las capacidades de su computador, usted puede ajustar la cantidad de datos para entrenamiento. Las etiquetas se guardan en un arreglo separado y contienen enteros entre 0 y 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_arrays(nb_rows, img_size):\n",
    "  if nb_rows:\n",
    "    dataset = np.ndarray((nb_rows, img_size, img_size), dtype=np.float32)\n",
    "    labels = np.ndarray(nb_rows, dtype=np.int32)\n",
    "  else:\n",
    "    dataset, labels = None, None\n",
    "  return dataset, labels\n",
    "\n",
    "def merge_datasets(pickle_files, train_size, valid_size=0):\n",
    "  num_classes = len(pickle_files)\n",
    "  valid_dataset, valid_labels = make_arrays(valid_size, image_size)\n",
    "  train_dataset, train_labels = make_arrays(train_size, image_size)\n",
    "  vsize_per_class = valid_size // num_classes\n",
    "  tsize_per_class = train_size // num_classes\n",
    "    \n",
    "  start_v, start_t = 0, 0\n",
    "  end_v, end_t = vsize_per_class, tsize_per_class\n",
    "  end_l = vsize_per_class+tsize_per_class\n",
    "  for label, pickle_file in enumerate(pickle_files):       \n",
    "    try:\n",
    "      with open(pickle_file, 'rb') as f:\n",
    "        letter_set = pickle.load(f)\n",
    "        #Mezclar los datos de una misma clase para que se presenten de forma aleatoria\n",
    "        np.random.shuffle(letter_set)\n",
    "        if valid_dataset is not None:\n",
    "          valid_letter = letter_set[:vsize_per_class, :, :]\n",
    "          valid_dataset[start_v:end_v, :, :] = valid_letter\n",
    "          valid_labels[start_v:end_v] = label\n",
    "          start_v += vsize_per_class\n",
    "          end_v += vsize_per_class\n",
    "                    \n",
    "        train_letter = letter_set[vsize_per_class:end_l, :, :]\n",
    "        train_dataset[start_t:end_t, :, :] = train_letter\n",
    "        train_labels[start_t:end_t] = label\n",
    "        start_t += tsize_per_class\n",
    "        end_t += tsize_per_class\n",
    "    except Exception as e:\n",
    "      print('No se pudo procesar datos de: ', pickle_file, ':', e)\n",
    "      raise\n",
    "    \n",
    "  return valid_dataset, valid_labels, train_dataset, train_labels\n",
    "            \n",
    "            \n",
    "train_size = 200000\n",
    "valid_size = 10000\n",
    "test_size = 10000\n",
    "\n",
    "valid_dataset, valid_labels, train_dataset, train_labels = merge_datasets(\n",
    "  train_datasets, train_size, valid_size)\n",
    "_, _, test_dataset, test_labels = merge_datasets(test_datasets, test_size)\n",
    "\n",
    "print('Entrenamiento:', train_dataset.shape, train_labels.shape)\n",
    "print('Validacion:', valid_dataset.shape, valid_labels.shape)\n",
    "print('Prueba:', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego se mezcla de forma aleatoria todos los datos de todas las clases. Es importante tener los datos bien distribuidos para evitar sesgo en diferentes etapas de entrenamiento\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def randomize(dataset, labels):\n",
    "  permutation = np.random.permutation(labels.shape[0])\n",
    "  shuffled_dataset = dataset[permutation,:,:]\n",
    "  shuffled_labels = labels[permutation]\n",
    "  return shuffled_dataset, shuffled_labels\n",
    "train_dataset, train_labels = randomize(train_dataset, train_labels)\n",
    "test_dataset, test_labels = randomize(test_dataset, test_labels)\n",
    "valid_dataset, valid_labels = randomize(valid_dataset, valid_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problema 3\n",
    "==========\n",
    "El estudiante debe verificar que en este punto las etiquetas corresponden con las imagenes. Debe mostrar 10 muestras de cada clase despues de haber sido mezcladas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IM=np.zeros([2,282])\n",
    "models = np.unique(train_labels)\n",
    "idx    = np.arange(train_labels.size)\n",
    "for mm in models:    \n",
    "    xx_root=np.zeros([28,2])     \n",
    "    idx_mm = idx[train_labels == mm]    \n",
    "    for ii in range(10):\n",
    "         xx_root=np.hstack((xx_root,train_dataset[idx_mm[ii],:,:]))\n",
    "    IM=np.vstack((IM,xx_root))\n",
    "plt.imshow(IM,cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
